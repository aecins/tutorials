{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aecins/tutorials/blob/main/least_squares/ordinary_least_squares.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2263e7ab",
      "metadata": {
        "id": "2263e7ab"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import scipy.linalg"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b6f3ee3a",
      "metadata": {
        "id": "b6f3ee3a"
      },
      "source": [
        "## Create a linear least squares problem ([wiki](https://en.wikipedia.org/wiki/Linear_regression))\n",
        "We setup a simple linear least squares problem by creating values for:\n",
        "- true parameter values $x$ (sampled from uniform distribution)\n",
        "- independent variables $A$ (sampled from uniform distribution)\n",
        "- dependent variables $b$ (constructed as $Ax + \\mathcal{N}(0, I * \\sigma_b)$, i.e. residuals are uncorrelated and have the same variance)\n",
        "\n",
        "This satisfies the assumptions on the measurement errors of ordinary least squares:\n",
        "- Measurement errors are uncorrelated\n",
        "- Measurement errors have the same variance ([homoscedasticity](https://en.wikipedia.org/wiki/Homoscedasticity))\n",
        "- Measurement errors are zero mean normally distributed\n",
        "\n",
        "These assumptions can be summarized as a single assumption:\n",
        "- measurement error vector is drawn from a multivariate Gaussian distribution with zero mean and a covariance matrix of $I * \\sigma_b^2$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "395016c8",
      "metadata": {
        "id": "395016c8"
      },
      "outputs": [],
      "source": [
        "# Function that generates random values drawn from uniform distribution with a given range.\n",
        "def generate_random_uniform(y_size, x_size, value_min, value_max):\n",
        "    assert(value_max > value_min)\n",
        "\n",
        "    values = np.random.rand(y_size, x_size)\n",
        "    values = values * (value_max - value_min)\n",
        "    values = values + value_min\n",
        "\n",
        "    return values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0911e759",
      "metadata": {
        "id": "0911e759",
        "outputId": "4e10a05c-4af8-4230-c29d-dbfc36c747b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[ 6.96416589]\n",
            " [ 6.21106091]\n",
            " [ 8.21836152]\n",
            " [-7.4435199 ]]\n"
          ]
        }
      ],
      "source": [
        "# Create true parameters of a linear least squares problem by sampling them uniformly\n",
        "# in the range [-X_TRUE_MAGNITUDE; X_TRUE_MAGNITUDE]\n",
        "NUM_PARAMETERS = 4\n",
        "# NUM_PARAMETERS = 2000\n",
        "X_TRUE_MAGNITUDE = 10\n",
        "\n",
        "x_true = generate_random_uniform(NUM_PARAMETERS, 1, -X_TRUE_MAGNITUDE, X_TRUE_MAGNITUDE)\n",
        "print(x_true)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "05b98665",
      "metadata": {
        "id": "05b98665"
      },
      "outputs": [],
      "source": [
        "# Function that generates random independent variables (A) and corresponding noisy dependent variables (b)\n",
        "# for a linear least squares problem.\n",
        "def generate_Ab(x_true, num_measurements, A_magnitude, b_noise_sigma):\n",
        "    # First generate independent variables aka A matrix.\n",
        "    # These are generated from a uniform distribution in the range [-A_magnitude; A_magnitude]\n",
        "    A = generate_random_uniform(NUM_MEASUREMENTS, NUM_PARAMETERS, -A_magnitude, A_magnitude)\n",
        "\n",
        "    # Next generate dependent variables aka vector b.\n",
        "    # These are generated as values predicted by the true values of the model + measurement noise\n",
        "    measurement_noise = (np.random.normal(0, b_noise_sigma, [num_measurements, 1]))\n",
        "    b = A.dot(x_true) + measurement_noise\n",
        "\n",
        "    return [A, b]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8aa98c55",
      "metadata": {
        "id": "8aa98c55"
      },
      "outputs": [],
      "source": [
        "# Generate measurements.\n",
        "NUM_MEASUREMENTS = 100000\n",
        "A_MAGNITUDE = 10\n",
        "B_NOISE_SIGMA = 10000\n",
        "\n",
        "[A, b] = generate_Ab(x_true, NUM_MEASUREMENTS, A_MAGNITUDE, B_NOISE_SIGMA)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd1f9734",
      "metadata": {
        "id": "bd1f9734"
      },
      "source": [
        "## Validate residuals\n",
        "The vector of residuals is defined as:\n",
        "$$\n",
        "r = Ax - b\n",
        "$$\n",
        "We can check that the system was constructed correctly by checking that residuals evaluated at ground truth solution are distributed the same was as the noise that was added to dependent measurements $b$:\n",
        "$$\n",
        "b = Ax_{true} + \\mathcal{N}(0, \\sigma_b) \\\\\n",
        "r = Ax_{true} - b \\\\\n",
        "r \\sim \\mathcal{N}(0, \\sigma_b)\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b368c587",
      "metadata": {
        "scrolled": false,
        "id": "b368c587",
        "outputId": "6967e850-0cf9-4f16-967e-f8a8a8cd74bc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Statistics of residuals evaluated at true parameter values\n",
            "mean  : estimated -16.428822, expected 0.000000\n",
            "sigma : estimated 9995.438988, expected 10000.000000\n"
          ]
        }
      ],
      "source": [
        "# Check that the residuals evaluated at true parameter values have the same statistics as measurement noise.\n",
        "residuals_true = A.dot(x_true) - b\n",
        "print(\"Statistics of residuals evaluated at true parameter values\")\n",
        "print(\"mean  : estimated {:f}, expected {:f}\".format(np.mean(residuals_true), 0))\n",
        "print(\"sigma : estimated {:f}, expected {:f}\".format(np.std(residuals_true), B_NOISE_SIGMA))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3b9cc651",
      "metadata": {
        "id": "3b9cc651"
      },
      "source": [
        "## Jacobian\n",
        "Jacobian of a vector-valued function is defined as a matrix of partial derivatives of the value of the function with respect to its parameters [Jacobian](https://en.wikipedia.org/wiki/Jacobian_matrix_and_determinant). In the context of optimization we are interested in the Jacobian of the residuals since the tells us how the residuals change as we change the parameters.\n",
        "\n",
        "For a linear least squares problem we have:\n",
        "$$\n",
        "\\begin{align*}\n",
        "r(x) & = Ax - b \\\\\n",
        "J & = \\begin{bmatrix}\\dfrac{dr_i}{dx_j}\\end{bmatrix} = A\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "### Validating Jacobian\n",
        "For any linear function $f(x)$ with Jacobian $J$ we can predict the value of a function at one point given the value of the function at another point:\n",
        "$$\n",
        "f(x) = f(y) + J * (x - y) = f(y) + J \\Delta\n",
        "$$\n",
        "(note: $\\Delta = x-y$ is a vector and $J * (x - y)$ is a matrix vector product.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11e0cb35",
      "metadata": {
        "id": "11e0cb35",
        "outputId": "601e5a61-4fea-4d0b-b47c-c79560ea9e74"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Maximum norm of the difference between true and predicted values of the residual: 0.000\n"
          ]
        }
      ],
      "source": [
        "# Validate Jacobian of the residual function.\n",
        "NUM_JACOBIAN_TESTS = 100\n",
        "max_value_difference = 0\n",
        "\n",
        "# Calculate Jacobian.\n",
        "J = A\n",
        "\n",
        "for i in range(NUM_JACOBIAN_TESTS):\n",
        "    # Create two random parameter values.\n",
        "    x = generate_random_uniform(NUM_PARAMETERS, 1, -X_TRUE_MAGNITUDE, X_TRUE_MAGNITUDE)\n",
        "    y = generate_random_uniform(NUM_PARAMETERS, 1, -X_TRUE_MAGNITUDE, X_TRUE_MAGNITUDE)\n",
        "\n",
        "    # Calculate residuals\n",
        "    r_x = A.dot(x) - b\n",
        "    r_y = A.dot(y) - b\n",
        "\n",
        "    # Calculate predicted residual\n",
        "    delta = x - y\n",
        "    r_x_predicted = r_y + J.dot(delta)\n",
        "    max_value_difference = max(max_value_difference, np.linalg.norm(r_x - r_x_predicted))\n",
        "\n",
        "print(\"Maximum norm of the difference between true and predicted values of the residual: {:.3f}\".format(max_value_difference))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0d4b055a",
      "metadata": {
        "id": "0d4b055a"
      },
      "source": [
        "## Normal equations\n",
        "An optimal solution $x^*$ to the linear least squares problem is the solution to the system of linear equations called the [normal equations](https://en.wikipedia.org/wiki/Ordinary_least_squares#:~:text=A%20justification%20for,the%20normal%20equations):\n",
        "$$A^TA x^* = A^Tb$$\n",
        "\n",
        "### Information matrix / Hessian\n",
        "$A^TA$ is also known as the information matrix $\\mathcal{I}$. It can also be interpreted as the matrix of second order partial derivatives of the least squares cost function with respect to parameters $x$. In other words $A^TA$ is the [Hessian](https://en.wikipedia.org/wiki/Hessian_matrix) of the cost function:\n",
        "$$\n",
        "f(x) = ||Ab - b||^2 \\\\\n",
        "H = \\begin{bmatrix} \\dfrac{\\partial f}{\\partial x_i  \\partial x_j} \\end{bmatrix} = A^TA\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "## Solving by inverting the information matrix\n",
        "Optimal solution to the problem can be found by inverting the information matrix:\n",
        "\n",
        "$$x^* = (A^T A)^{-1} A^T b$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "98735167",
      "metadata": {
        "id": "98735167",
        "outputId": "20913d89-4345-4665-c38e-6c449490e041"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 1.38 ms, sys: 1.78 ms, total: 3.16 ms\n",
            "Wall time: 1.63 ms\n"
          ]
        }
      ],
      "source": [
        "# Solve linear least squares problem by inverting the information matrix.\n",
        "def solve_using_matrix_inverse(A, b):\n",
        "    I = A.transpose().dot(A)\n",
        "    I_inv = np.linalg.inv(I)\n",
        "    Atb = (A.transpose()).dot(b)\n",
        "    return np.matmul(I_inv, Atb)\n",
        "\n",
        "%time x_estimated = solve_using_matrix_inverse(A, b)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c8820bd1",
      "metadata": {
        "id": "c8820bd1",
        "outputId": "f2cd730d-74d1-4642-fc02-131106327bea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Root Mean Squared Error of paramters estimated using information matrix inverse:\n",
            " 1.995826\n"
          ]
        }
      ],
      "source": [
        "# Calculate RMSE of parameters.\n",
        "def parameter_rmse(x_estimated, x_true):\n",
        "    assert(len(x_estimated) == len(x_true))\n",
        "    return np.linalg.norm(x_estimated - x_true) / len(x_estimated)\n",
        "\n",
        "rmse = parameter_rmse(x_estimated, x_true)\n",
        "print(\"Root Mean Squared Error of paramters estimated using information matrix inverse:\\n {:f}\".format(rmse))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2936224d",
      "metadata": {
        "id": "2936224d"
      },
      "source": [
        "## Solve using factorization of the information matrix\n",
        "Using least squares problems by inverting the information matrix has several downsides:\n",
        "- inverting large matrices is slow\n",
        "- inverting matrices is numerically unstable\n",
        "\n",
        "Alternatively, we can take advantage of the special structure of the information matrix. Since $\\mathcal{I} = A^TA$ is a real summetric matrix we can factorize it using the [LDL decomposition](https://en.wikipedia.org/wiki/Cholesky_decomposition#LDL_decomposition):\n",
        "$$A^TA = LDL^T$$\n",
        "where $L$ is a lower diagonal matrix with unit diagonal and $D$ is a diagonal matrix. Normal equations become:\n",
        "$$\n",
        "LDL^Tx^* = A^Tb\n",
        "$$\n",
        "The solution can be obtained by solving a series of [systems of linear equations](https://en.wikipedia.org/wiki/System_of_linear_equations) (not to be confused with linear least squares problems!):\n",
        "1. $Ly_1 = A^Tb$ (solved using [forward substituition](https://en.wikipedia.org/wiki/Triangular_matrix#Forward_and_back_substitution) since $L$ is lower triangular)\n",
        "1. $Dy_2 = y_1$ (solved by element-wise division since $D$ is a diagonal matrix)\n",
        "1. $L^Tx^* = y_2$ (solved using [back substituition](https://en.wikipedia.org/wiki/Triangular_matrix#Forward_and_back_substitution) since $L^T$ is upper triangular)\n",
        "\n",
        "Note that none of the steps in the solution (calculating $A^TA$ and $A^Tb$, computing the LDL factorization of $A^TA$) and solving the resulting linear system of equations) require expensive operations like taking matrix inverse, squaring or taking square roots. In fact they only requires multiply-adds. This makes this approach both fast and numerically stable.\n",
        "\n",
        "$LDL^T$ solver is implemeted in the [Eigen C++ library](https://eigen.tuxfamily.org/dox/classEigen_1_1LDLT.html#aa257dd7a8acf8b347d5a22a13d6ca3e1)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca39a52c",
      "metadata": {
        "id": "ca39a52c",
        "outputId": "328ee58d-9e41-439f-a9fd-fdab8743d2af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 1.39 ms, sys: 1.78 ms, total: 3.18 ms\n",
            "Wall time: 11.1 ms\n",
            "Root Mean Squared Error of paramters estimated using LDL factorization:\n",
            " 1.995826\n"
          ]
        }
      ],
      "source": [
        "# Solve linear least squares problem using LDL factorization.\n",
        "def solve_using_ldl(A, b):\n",
        "    # Calculate LDL factorization of the information matrix\n",
        "    I = A.transpose().dot(A)\n",
        "    [L, D, perm] = scipy.linalg.ldl(I)\n",
        "\n",
        "    # Solve Ly_1 = A^Tb\n",
        "    y1 = scipy.linalg.solve_triangular(L, A.transpose().dot(b), trans=0, lower=True, unit_diagonal=True)\n",
        "\n",
        "    # Solve Dy2 = y1\n",
        "    diagonal = np.diagonal(D).reshape(-1, 1)\n",
        "    y2 = np.divide(y1, diagonal)\n",
        "\n",
        "    # Solve L^Tx^* = y_2\n",
        "    return scipy.linalg.solve_triangular(L.transpose(), y2, trans=0, lower=False, unit_diagonal=True)\n",
        "\n",
        "%time x_estimated = solve_using_ldl(A, b)\n",
        "rmse = parameter_rmse(x_estimated, x_true)\n",
        "print(\"Root Mean Squared Error of paramters estimated using LDL factorization:\\n {:f}\".format(rmse))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d25ef9c",
      "metadata": {
        "id": "7d25ef9c"
      },
      "source": [
        "#### Note on the runtime\n",
        "The algorithm used for calcularing matrix inverse in `numpy.linalg.inv` is optimized for performance. Hence we don't see a large improvement in the runtime between inverse information matrix and LDLT solvers."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d744c68f",
      "metadata": {
        "id": "d744c68f"
      },
      "source": [
        "## Solution covariance\n",
        "### Estimating covariance using information matrix\n",
        "Given a linear least squares problem $Ax = b$ the covariance matrix of the solution can be estimated as:\n",
        "$$cov(x^*) = (A^T A)^{-1} \\sigma_b^2$$\n",
        "where $\\sigma_b$ is the standard deviation of the residuals ([link](https://en.wikipedia.org/wiki/Ordinary_least_squares#:~:text=The%20variance-covariance,sigma%20%5E%7B2%7DQ.%7D))([proof](https://github.com/aecins/Least-Squares-Notebooks/blob/main/least_squares_covariance_derivation.ipynb)).\n",
        "\n",
        "We expect the covariance matrix to be approximately equal to $I * \\alpha$ for some scaling factor $\\alpha$. This is because all of the measurements in $A$ and $b$ are independent (hence cross-covariance terms should be zero)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6bc51f7e",
      "metadata": {
        "id": "6bc51f7e",
        "outputId": "b2247305-45d1-4c78-bd4d-3336ae63a520"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Solution covariance estimated using A matrix from a single problem:\n",
            "[[ 30.018 -0.015  0.053  0.043]\n",
            " [-0.015  30.186 -0.100 -0.183]\n",
            " [ 0.053 -0.100  29.928 -0.035]\n",
            " [ 0.043 -0.183 -0.035  30.072]]\n"
          ]
        }
      ],
      "source": [
        "# Calculate the covariance matrix of the estimated parameters.\n",
        "x_estimated_covariance = np.linalg.inv(np.matmul(A.transpose(), A)) * B_NOISE_SIGMA * B_NOISE_SIGMA\n",
        "print(\"Solution covariance estimated using A matrix from a single problem:\")\n",
        "np.set_printoptions(formatter={'float': '{: 0.3f}'.format})\n",
        "print(x_estimated_covariance)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ac8b488",
      "metadata": {
        "id": "5ac8b488"
      },
      "source": [
        "### Estimating covariane using bootstrap\n",
        "Another way to estimate covariance of the solution is to use a bootstrap method. The high-level idea of this method is to:\n",
        "1. generate multiple optimization problems that are \"equivalent\" to the original problem\n",
        "2. solve these problems and get multiple estimates of the solution $x$\n",
        "3. calculate the covariance matrix of the obtained solutionsand then calculate\n",
        "\n",
        "Multiple ways were proposed to generate \"equivalent\" optimization problems ([wiki](https://en.wikipedia.org/wiki/Bootstrapping_(statistics))). One way is to resample with replacement the constraints in the optimization problems. That corresponds to resampling the rows in the matrix $A$ and corresponding values in $b$. In our case, since we are using synthetic data that is generated from a known distribution - we can simply generate multiple problems that:\n",
        "- have the same solution $x$\n",
        "- values in $A$ and $b$ are drawn from the same distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a82fcb6",
      "metadata": {
        "id": "0a82fcb6",
        "outputId": "c222b799-0999-4b54-8460-237786a709a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Solution covariance estimated using bootstrap:\n",
            "[[ 30.407  0.058 -0.178 -0.661]\n",
            " [ 0.058  30.605  0.263 -0.090]\n",
            " [-0.178  0.263  30.350 -0.476]\n",
            " [-0.661 -0.090 -0.476  30.419]]\n",
            "CPU times: user 1min 50s, sys: 6.37 s, total: 1min 57s\n",
            "Wall time: 1min 57s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# Calculate the covariance matrix using bootstrap.\n",
        "# We create multiple problems with the same true solution but different measurements (A, b)(same measurement noise).\n",
        "# We then calculate the covariance matrix of the solutions to these problems.\n",
        "NUM_BOOTSTRAP_ITERATIONS = 10000\n",
        "x_estimates = np.zeros((NUM_PARAMETERS, NUM_BOOTSTRAP_ITERATIONS))\n",
        "for i in range(NUM_BOOTSTRAP_ITERATIONS):\n",
        "    # Generate A and b.\n",
        "    [A, b] = generate_Ab(x_true, NUM_MEASUREMENTS, A_MAGNITUDE, B_NOISE_SIGMA)\n",
        "\n",
        "    # Solve.\n",
        "#     x_estimated = np.linalg.lstsq(A, b, rcond=None)[0]\n",
        "    x_estimated = solve_using_matrix_inverse(A, b)\n",
        "\n",
        "    # Append to solutions\n",
        "    x_estimates[:, i] = x_estimated[:, 0]\n",
        "\n",
        "print(\"Solution covariance estimated using bootstrap:\")\n",
        "print(np.cov(x_estimates))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f21858b",
      "metadata": {
        "id": "4f21858b"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}