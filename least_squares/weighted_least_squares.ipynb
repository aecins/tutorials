{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aecins/tutorials/blob/main/least_squares/weighted_least_squares.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33d16721",
      "metadata": {
        "id": "33d16721"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "np.set_printoptions(formatter={'float': '{: 0.3f}'.format})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4cd9b12b",
      "metadata": {
        "id": "4cd9b12b"
      },
      "source": [
        "# Create a weigted least squares problem ([wiki](https://en.wikipedia.org/wiki/Weighted_least_squares))\n",
        "We setup a simple linear least squares problem by creating values for:\n",
        "- true parameter values $x$ (sampled from uniform distribution)\n",
        "- independent variables $A$ (sampled from uniform distribution)\n",
        "- dependent variables $b$ (constructed as $Ax + \\mathcal{N}(0, \\Sigma)$ where $\\Sigma$ is a diagonal matrix i.e. residuals are uncorrelated but have different variances)\n",
        "$$\n",
        "\\Sigma =\n",
        "\\begin{bmatrix}\n",
        "\\sigma_0^2 & \\cdots & 0 \\\\\n",
        "\\vdots & \\ddots & \\vdots \\\\\n",
        "0 & \\cdots & \\sigma_n^2 \\\\\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "This satisfies the assumptions on the measurement errors of ordinary least squares:\n",
        "- Measurement errors are uncorrelated\n",
        "- Measurement errors have different variance ([heteroscedasticity](https://en.wikipedia.org/wiki/Heteroscedasticity))\n",
        "- Measurement errors are zero mean normally distributed\n",
        "\n",
        "These assumptions can be summarized as a single assumption:\n",
        "- measurement error vector is drawn from a multivariate Gaussian distribution with zero mean and a diagonal covariance matrix of $\\Sigma$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3fca220b",
      "metadata": {
        "id": "3fca220b"
      },
      "outputs": [],
      "source": [
        "# Function that generates random values drawn from uniform distribution with a given range.\n",
        "def generate_random_uniform(y_size, x_size, value_min, value_max):\n",
        "    assert(value_max > value_min)\n",
        "\n",
        "    values = np.random.rand(y_size, x_size)\n",
        "    values = values * (value_max - value_min)\n",
        "    values = values + value_min\n",
        "\n",
        "    return values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "61c352a0",
      "metadata": {
        "id": "61c352a0",
        "outputId": "43828911-63aa-4c65-bb81-edf70dbc35e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[-2.941]\n",
            " [-3.764]\n",
            " [ 1.385]\n",
            " [-2.768]]\n"
          ]
        }
      ],
      "source": [
        "# Create true parameters of a linear least squares problem by sampling them uniformly\n",
        "# in the range [-X_TRUE_MAGNITUDE; X_TRUE_MAGNITUDE]\n",
        "NUM_PARAMETERS = 4\n",
        "X_TRUE_MAGNITUDE = 10\n",
        "\n",
        "x_true = generate_random_uniform(NUM_PARAMETERS, 1, -X_TRUE_MAGNITUDE, X_TRUE_MAGNITUDE)\n",
        "print(x_true)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9835cdfd",
      "metadata": {
        "id": "9835cdfd"
      },
      "outputs": [],
      "source": [
        "# Function that generates random independent variables (A) and corresponding noisy dependent variables (b)\n",
        "# for a linear least squares problem.\n",
        "def generate_Ab(x_true, num_measurements, A_magnitude, b_noise_sigmas):\n",
        "    # First generate independent variables aka A matrix.\n",
        "    # These are generated from a uniform distribution in the range [-A_magnitude; A_magnitude]\n",
        "    A = generate_random_uniform(NUM_MEASUREMENTS, NUM_PARAMETERS, -A_magnitude, A_magnitude)\n",
        "\n",
        "    # Next generate dependent variables aka vector b.\n",
        "    # These are generated as values predicted by the true values of the model + measurement noise\n",
        "    measurement_noise = (np.random.normal(0, b_noise_sigmas, [num_measurements, 1]))\n",
        "    b = A.dot(x_true) + measurement_noise\n",
        "\n",
        "    return [A, b]\n",
        "\n",
        "# Create measurement sigmas.\n",
        "NUM_MEASUREMENTS = 10000\n",
        "B_NOISE_SIGMA_MAGNITUDE = 1000000\n",
        "b_sigmas = generate_random_uniform(NUM_MEASUREMENTS, 1, 0, B_NOISE_SIGMA_MAGNITUDE)\n",
        "\n",
        "# Geneare measurements.\n",
        "A_MAGNITUDE = 10\n",
        "[A, b] = generate_Ab(x_true, NUM_MEASUREMENTS, A_MAGNITUDE, b_sigmas)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "872e9762",
      "metadata": {
        "id": "872e9762"
      },
      "source": [
        "# Validate residuals\n",
        "The noise added to each measurement is sampled from a zero mean Gaussian distribution with different sigmas. The set of all noise measurements can be considered to be a sample from a [Gaussian mixture model (GMM)](https://en.wikipedia.org/wiki/Mixture_model) where all components have zero mean and are equally probable. The mean and standard deviation of such Gaussian mixture model are:\n",
        "$$\n",
        "\\mu_{gmm} = 0 \\\\\n",
        "\\sigma_{gmm} = \\sqrt{\\dfrac{1}{N}\\sum_N \\sigma_i^2}\n",
        "$$\n",
        "(see this [link](https://stats.stackexchange.com/questions/16608/what-is-the-variance-of-the-weighted-mixture-of-two-gaussians) for proof).\n",
        "\n",
        "We can check that the system was constructed correctly by checking that residuals evaluated at ground truth solution are distributed the same way as the measurement noise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "36c14f08",
      "metadata": {
        "id": "36c14f08",
        "outputId": "772295fa-6e79-43a5-ca12-9adf882192de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Statistics of residuals evaluated at true parameter values\n",
            "mean  : estimated -4342.408709, expected 0.000000\n",
            "sigma : estimated 576235.420587, expected 572802.760831\n"
          ]
        }
      ],
      "source": [
        "# Check that the residuals evaluated at true parameter values have the same statistics as measurement noise.\n",
        "# NOTE: accuracy of the mean and standard deviation estimates depends on NUM_PARAMETERS.\n",
        "residuals_true = A.dot(x_true) - b\n",
        "residuals_expected_mean = 0\n",
        "residuals_expected_sigma = np.sqrt(np.mean(np.square(b_sigmas)))\n",
        "print(\"Statistics of residuals evaluated at true parameter values\")\n",
        "print(\"mean  : estimated {:f}, expected {:f}\".format(np.mean(residuals_true), residuals_expected_mean))\n",
        "print(\"sigma : estimated {:f}, expected {:f}\".format(np.std(residuals_true), residuals_expected_sigma))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b084f103",
      "metadata": {
        "id": "b084f103"
      },
      "source": [
        "# Solve using ordinary least squares\n",
        "We can attempt to solve the problem using ordinary least squares."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "745d62b0",
      "metadata": {
        "id": "745d62b0"
      },
      "outputs": [],
      "source": [
        "# Calculate RMSE of parameters.\n",
        "def parameter_rmse(x_estimated, x_true):\n",
        "    assert(len(x_estimated) == len(x_true))\n",
        "    return np.linalg.norm(x_estimated - x_true) / len(x_estimated)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "706001f4",
      "metadata": {
        "id": "706001f4"
      },
      "outputs": [],
      "source": [
        "# Calculate covariance of solution using bootstrap.\n",
        "NUM_BOOTSTRAP_ITERATIONS = 10000\n",
        "def bootstrap_covariance(solver_function):\n",
        "    x_estimates = np.zeros((NUM_PARAMETERS, NUM_BOOTSTRAP_ITERATIONS))\n",
        "    for i in range(NUM_BOOTSTRAP_ITERATIONS):\n",
        "        # Generate A and b.\n",
        "        [A, b] = generate_Ab(x_true, NUM_MEASUREMENTS, A_MAGNITUDE, b_sigmas)\n",
        "\n",
        "        # Solve.\n",
        "        x_estimated = solver_function(A, b, b_sigmas)\n",
        "\n",
        "        # Append to solutions\n",
        "        x_estimates[:, i] = x_estimated[:, 0]\n",
        "\n",
        "    return np.cov(x_estimates)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3b6e84c",
      "metadata": {
        "id": "e3b6e84c",
        "outputId": "b387638d-204c-4fee-8c62-35e3885fdc34"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Root mean Squared Error of paramters estimated using OLS solution:\n",
            " 637.022794\n",
            "Average residual:\n",
            " -4401.073490\n",
            "Solution covariance estimated using bootstrap:\n",
            "[[ 972482.050 -19447.076 -13566.337  17088.933]\n",
            " [-19447.076  970392.787 -2460.736 -917.164]\n",
            " [-13566.337 -2460.736  1004987.336 -15665.740]\n",
            " [ 17088.933 -917.164 -15665.740  992996.862]]\n"
          ]
        }
      ],
      "source": [
        "# Solve linear least squares usig numpy solver.\n",
        "def solve_least_squares_numpy(A, b, b_noise_sigmas):\n",
        "    # NOTE: b_noise_sigmas are ignored\n",
        "    return np.linalg.lstsq(A, b, rcond=None)[0]\n",
        "\n",
        "x_estimated = solve_least_squares_numpy(A, b, b_sigmas)\n",
        "residuals_estimated = A.dot(x_estimated) - b\n",
        "rmse = parameter_rmse(x_estimated, x_true)\n",
        "print(\"Root mean Squared Error of paramters estimated using OLS solution:\\n {:f}\".format(rmse))\n",
        "print(\"Average residual:\\n {:f}\".format(np.mean(residuals_estimated)))\n",
        "\n",
        "# Calculate bootstrap covariance of the solution\n",
        "x_covariance = bootstrap_covariance(solve_least_squares_numpy)\n",
        "print(\"Solution covariance estimated using bootstrap:\")\n",
        "print(x_covariance)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "88df3a75",
      "metadata": {
        "id": "88df3a75"
      },
      "source": [
        "We see that using ordinary least squares method to solve this problem leads to:\n",
        "- high RMS error on the parameters\n",
        "- high solution covariance\n",
        "\n",
        "Intuitively this can be explained by the fact that optimization problem equally penalises residuals corresponding to constraints with high amount of noise and residuals with low noise. As a result the optimizer may choose a parameter estimate that allows a slightly increased residual for a low noise constraint in order to have a slightly lower residual on the high noise constraint. The true solution will have a very low residual on low noise contraint and a relatively high residual on the high noise contraint."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "99a2c1d8",
      "metadata": {
        "id": "99a2c1d8"
      },
      "source": [
        "# Solving weighted least squares\n",
        "## Residuals\n",
        "It was shown that the best estimate of the parameters for weighted least squares problem can be obtained by solving an ordinary least squares problem where the residuals are scaled by the inverse of the standard deviation of the corresponding measurement noise [[link](https://en.wikipedia.org/wiki/Weighted_least_squares#:~:text=Aitken%20showed%20that,of%20the%20measurement)]. The residual becomes:\n",
        "$$\n",
        "r_i = \\dfrac{1}{\\sigma_i}(Ax_i - b)\n",
        "$$\n",
        "Such residuals are called a **whitened residuals**. Whitened residuals evaluated at true parameter values are normally distributed with unit variance (assuming all assuptions of weighted least squares hold).\n",
        "\n",
        "If we construct the weight matrix $W$ to be the matrix inverse of the measurement covariance matrix $\\Sigma$ then we can express the residual vector in the matrix form:\n",
        "$$\n",
        "W = \\Sigma^{-1}  =\n",
        "\\begin{bmatrix}\n",
        "\\frac{1}{\\sigma_0^2} & \\cdots & 0 \\\\\n",
        "\\vdots & \\ddots & \\vdots \\\\\n",
        "0 & \\cdots & \\frac{1}{\\sigma_n^2} \\\\\n",
        "\\end{bmatrix} \\\\\n",
        "r = W^{\\frac{1}{2}}(Ax - b)\n",
        "$$\n",
        "\n",
        "## Whitening transform\n",
        "Consider a column vector containing all of the residuals to be a random variable $\\mathbf{r}$. This random variable has a covariance matrix $\\Sigma$. Transforming this variable by $W^{\\frac{1}{2}} = \\Sigma^{\\frac{1}{2}}$ creates a new variable $W^{\\frac{1}{2}}\\mathbf{r}$ that has an identity covariance matrix. A linear transformation that transforms a random variable to have unit covariance is known as a *[whitening transform](https://github.com/aecins/Least-Squares-Notebooks/blob/main/whitening_transform.ipynb)*."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fbffc18e",
      "metadata": {
        "id": "fbffc18e"
      },
      "source": [
        "## Jacobian\n",
        "Similarly to how we can derive whitened residuals in a weighted least squares problem - we can also derive the whitened Jacobian:\n",
        "$$\n",
        "\\begin{align*}\n",
        "r(x) & = W^{\\frac{1}{2}}(Ax - b) \\\\\n",
        "J & = \\begin{bmatrix}\\dfrac{dr_i}{dx_j}\\end{bmatrix} = W^{\\frac{1}{2}}A\n",
        "\\end{align*}\n",
        "$$\n",
        "Note that $W^{\\frac{1}{2}}A$ is equivalent to dividing each row of A by standard deviation of the corresponding measurement noise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff3a5bf5",
      "metadata": {
        "id": "ff3a5bf5",
        "outputId": "3049ab97-baa7-400c-99dd-a34ea360498b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Maximum norm of the difference between true and predicted values of the residual: 0.000\n"
          ]
        }
      ],
      "source": [
        "# Validate Jacobian of the residual function.\n",
        "NUM_JACOBIAN_TESTS = 100\n",
        "max_value_difference = 0\n",
        "\n",
        "# Calculate Jacobian.\n",
        "J = A / b_sigmas\n",
        "\n",
        "for i in range(NUM_JACOBIAN_TESTS):\n",
        "    # Create two random parameter values.\n",
        "    x = generate_random_uniform(NUM_PARAMETERS, 1, -X_TRUE_MAGNITUDE, X_TRUE_MAGNITUDE)\n",
        "    y = generate_random_uniform(NUM_PARAMETERS, 1, -X_TRUE_MAGNITUDE, X_TRUE_MAGNITUDE)\n",
        "\n",
        "    # Calculate residuals\n",
        "    r_x = (A.dot(x) - b) / b_sigmas\n",
        "    r_y = (A.dot(y) - b) / b_sigmas\n",
        "#     print(x - y)\n",
        "#     print(r_x - r_y)\n",
        "\n",
        "    # Calculate predicted residual\n",
        "    delta = x - y\n",
        "    r_x_predicted = r_y + (J.dot(delta))\n",
        "    max_value_difference = max(max_value_difference, np.linalg.norm(r_x - r_x_predicted))\n",
        "\n",
        "print(\"Maximum norm of the difference between true and predicted values of the residual: {:.3f}\".format(max_value_difference))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8a223797",
      "metadata": {
        "id": "8a223797"
      },
      "source": [
        "## Normal equations\n",
        "To solve weighted least squares problem we want to find a set of parameters $x^*$ that minimize the sum of squared whitened residuals:\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\DeclareMathOperator*{\\argmin}{arg\\,min}\n",
        "x^* & = \\argmin_x ||W^{\\frac{1}{2}}(Ax - b)||^2 \\\\\n",
        "    & = \\argmin_x ||W^{\\frac{1}{2}}Ax - W^{\\frac{1}{2}}b)||^2\n",
        "\\end{align*}\n",
        "$$\n",
        "This is equivalent to solving an ordinary least squares problem with the following substitution:\n",
        "$$\n",
        "\\begin{align*}\n",
        "A' & = W^{\\frac{1}{2}}A \\\\\n",
        "b' & = W^{\\frac{1}{2}}b\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "The normal equations for weighted least squares are can be obtained by plugging in expressions for $A'$ and $b'$ into the ordinary least squares normal equations:\n",
        "$$\n",
        "\\begin{align*}\n",
        "A'^T A' x^* & = A'^T b' \\\\\n",
        "A^T W A x^* & = A^T Wb\n",
        "\\end{align*}\n",
        "$$\n",
        "The optimal solution is obtained as:\n",
        "$$\n",
        "x^* = (A^T W A)^{-1}A^T Wb \\\\\n",
        "$$\n",
        "\n",
        "### Normal equations derivation\n",
        "$$\n",
        "\\begin{align*}\n",
        "A'^TA' x^* & = A'^T b' \\\\\n",
        "(W^{\\frac{1}{2}}A)^TW^{\\frac{1}{2}}A x^* & = (W^{\\frac{1}{2}}A)^T W^{\\frac{1}{2}}b \\\\\n",
        "(W^{\\frac{1}{2}}A)^TW^{\\frac{1}{2}}A x^* & = (W^{\\frac{1}{2}}A)^T W^{\\frac{1}{2}}b \\\\\n",
        "\\end{align*}\n",
        "$$\n",
        "Since $W$ is symmetric we have:\n",
        "$$\n",
        "(W^{\\frac{1}{2}}A)^T = A^T (W^{\\frac{1}{2}})^T  = A^T W^{\\frac{1}{2}}\n",
        "$$\n",
        "Plugging back we get the Normal equations for weighted least squares:\n",
        "$$\n",
        "\\begin{align*}\n",
        "A^T W^{\\frac{1}{2}} W^{\\frac{1}{2}}A x^* & = A^T W^{\\frac{1}{2}} W^{\\frac{1}{2}}b \\\\\n",
        "A^T W A x^* & = A^T Wb \\\\\n",
        "\\end{align*}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff9bed36",
      "metadata": {
        "id": "ff9bed36",
        "outputId": "c805805f-94f5-4173-dd73-76ca6e4c2f00"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Root mean Squared Error of paramters estimated using weighted least squares solution:\n",
            " 2.058716\n",
            "Average residual:\n",
            " -4342.449140\n",
            "Solution covariance estimated using bootstrap:\n",
            "[[ 179.534  1.314  1.237  1.347]\n",
            " [ 1.314  183.273 -0.230  0.326]\n",
            " [ 1.237 -0.230  181.422 -2.836]\n",
            " [ 1.347  0.326 -2.836  184.400]]\n"
          ]
        }
      ],
      "source": [
        "# Solve weighted linear least squares problem.\n",
        "def solve_weighted_least_squares(A, b, b_noise_sigmas):\n",
        "    # First whitened measurement matrices.\n",
        "    A_whitened = A / b_sigmas\n",
        "    b_whitened = b / b_sigmas\n",
        "\n",
        "    # Next solve an ordinary least squares problem with whitened measurements.\n",
        "    info = A_whitened.transpose().dot(A_whitened)\n",
        "    info_inv = np.linalg.inv(info)\n",
        "    Atb_whitened = (A_whitened.transpose()).dot(b_whitened)\n",
        "    return np.matmul(info_inv, Atb_whitened)\n",
        "\n",
        "x_estimated = solve_weighted_least_squares(A, b, b_sigmas)\n",
        "residuals_estimated = A.dot(x_estimated) - b\n",
        "rmse = parameter_rmse(x_estimated, x_true)\n",
        "print(\"Root mean Squared Error of paramters estimated using weighted least squares solution:\\n {:f}\".format(rmse))\n",
        "print(\"Average residual:\\n {:f}\".format(np.mean(residuals_estimated)))\n",
        "\n",
        "# Calculate bootstrap covariance of the solution\n",
        "x_covariance = bootstrap_covariance(solve_weighted_least_squares)\n",
        "print(\"Solution covariance estimated using bootstrap:\")\n",
        "print(x_covariance)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4de00038",
      "metadata": {
        "id": "4de00038"
      },
      "source": [
        "## Covariance\n",
        "Given a weighted least squares problem the covariance matrix of the solution can be estimated as:\n",
        "$$cov(x^*) = (A^T \\Sigma^{-1} A)^{-1}$$\n",
        "where $\\Sigma$ is the standard deviation of the residuals [[link](https://en.wikipedia.org/wiki/Weighted_least_squares#:~:text=When%20W%20%3D%20M%E2%88%921%2C%20this%20simplifies%20to)][[proof](https://github.com/aecins/Least-Squares-Notebooks/blob/main/least_squares_covariance_derivation.ipynb)]."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65e49a6a",
      "metadata": {
        "id": "65e49a6a",
        "outputId": "c6d67c68-7dcb-4271-fea6-252d33e51b65"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[ 219.170  46.953 -112.475 -79.246]\n",
            " [ 46.953  165.369 -42.285  1.161]\n",
            " [-112.475 -42.285  268.194  124.747]\n",
            " [-79.246  1.161  124.747  119.841]]\n"
          ]
        }
      ],
      "source": [
        "# Calculate covariance of the weighted least squares problem.\n",
        "A_prime = A / b_sigmas\n",
        "cov_inverse = A_prime.transpose().dot(A_prime)\n",
        "cov = np.linalg.inv(cov_inverse)\n",
        "print(cov)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e4e0dc6",
      "metadata": {
        "id": "0e4e0dc6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5983749a",
      "metadata": {
        "id": "5983749a"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}